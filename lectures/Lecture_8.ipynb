{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VNIH93hD-Ya"
      },
      "source": [
        "# Lecture 8 - PyTorch\n",
        "\n",
        "This will be the final lecture, today we will first have a brief introduction of deep learning, then we will look at some basics of using PyTorch to implement some simple models in deep learning.\n",
        "\n",
        "1. Homework due today"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJYsdl4IbsO6"
      },
      "source": [
        "# Basic Section (Start)\n",
        "What is Neural Network:\n",
        "1. [Recommended youtube video with great visual helpers](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)\n",
        "2.  Slides on course site\n",
        "# Basic Section (End)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-fCZYu2D-Yc"
      },
      "source": [
        "## Deep Learning Libraries\n",
        "\n",
        "There are many deep learning libraries available, the most common ones for Python are\n",
        "\n",
        "- TensorFlow, Keras\n",
        "- PyTorch\n",
        "\n",
        "Working with TensorFlow requires going into lot of details of the contruction of the computation graph, whereas Keras is a higher level interface for TensorFlow. Tensorflow is very popular in the industry and good for production code.\n",
        "\n",
        "PyTorch can be used as low level interface, but is much more user-friendly than TensorFlow, but it also has a higher level interface. PyTorch is more popular in the research community.\n",
        "\n",
        "In Stanford courses and projects, it is very common to use PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzUDYHi8D-Yd"
      },
      "source": [
        "## Main features that any deep learning library should provide\n",
        "\n",
        "No matter what library or language you use, the main features provided by a deep learning library are\n",
        "1. Use the GPU to speed up computation\n",
        "2. Ability to do automatic differentiation\n",
        "3. Useful library functions for common architectures and optimization algorithms\n",
        "\n",
        "### PyTorch\n",
        "We will look at all of the above in pytorch.\n",
        "The best way to think about pytorch is that its numpy + GPU + autograd.\n",
        "\n",
        "You can install it with\n",
        "\n",
        "```conda install pytorch```.\n",
        "\n",
        "Alternatively (and recommended), run this notebook in Google Colab-- it provides an environment with all of the PyTorch dependencies plus a GPU free of charge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCdvNHW0D-Ye"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpWzZewHD-Yi"
      },
      "source": [
        "The equivalent object to numpy arrays in PyTorch are called tensors, but they are just multidimensional arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t78yenP1D-Yj"
      },
      "outputs": [],
      "source": [
        "torch.tensor([2,3,4,5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Efg1UeizD-Ym"
      },
      "outputs": [],
      "source": [
        "torch.zeros((5,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BlufhDpD-Yp"
      },
      "outputs": [],
      "source": [
        "x = torch.ones((5,5))\n",
        "print(type(x))\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acgFdW_4D-Yr"
      },
      "outputs": [],
      "source": [
        "2*x + 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwCz7O1wD-Yu"
      },
      "outputs": [],
      "source": [
        "torch.randn(5,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fHiY5VKD-Yw"
      },
      "outputs": [],
      "source": [
        "x = torch.rand(25)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_QKyI7hD-Yz"
      },
      "outputs": [],
      "source": [
        "x=x.reshape(-1,5)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLKjs14-D-Y3"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn9fwJoSD-Y5"
      },
      "outputs": [],
      "source": [
        "print(torch.arange(10))\n",
        "print(torch.eye(5))\n",
        "print(torch.linspace(0,1,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cix7EXwSD-Y7"
      },
      "source": [
        "Some functions are a bit different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRrYVBDmjLpq"
      },
      "outputs": [],
      "source": [
        "A = np.random.rand(5,5)\n",
        "x = np.ones((5,1))\n",
        "print(A)\n",
        "print(x)\n",
        "\n",
        "A@x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npGWD47jicvY"
      },
      "outputs": [],
      "source": [
        "A_ = torch.rand((5,5))\n",
        "x_ = torch.ones(5,1)\n",
        "print(A_)\n",
        "print(x_)\n",
        "A_@x_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oz9QB17QTCL"
      },
      "outputs": [],
      "source": [
        "# class Test:\n",
        "#   def __init__(self):\n",
        "#     self.__data = 'Great__'\n",
        "#     self.data = 'Great'\n",
        "# test = Test()\n",
        "# print(test.__data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.ones((5,1))\n",
        "print(x)"
      ],
      "metadata": {
        "id": "2Y1aKfSTKOFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BukQIL5D-Y8"
      },
      "outputs": [],
      "source": [
        "x_ = torch.ones(5,1)\n",
        "print(x_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpEwcH-JD-ZA"
      },
      "source": [
        "You can convert tensors to a numpy array that shares its memory with the pytorch tensor -> to use more library that are compatible to numpy but not pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MquNPK71D-ZC"
      },
      "outputs": [],
      "source": [
        "x = torch.ones(5,5)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOeMqFrOD-ZE"
      },
      "outputs": [],
      "source": [
        "xn = x.numpy()\n",
        "print(type(xn))\n",
        "xn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZlG0x9xD-ZH"
      },
      "outputs": [],
      "source": [
        "# Changes in Numpy will cause changes in Tensor\n",
        "xn[4,2]=10\n",
        "xn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN6qJIsID-ZJ"
      },
      "outputs": [],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDnSshCXD-ZL"
      },
      "source": [
        "### Using the GPU\n",
        "\n",
        "The GPU (Graphical Processing Unit) is a separate processing unit that is specialized to handle bulk computations required for rendering high quality graphics. It mainly consists of a large number of processor cores that are individually very slow, but because of their sheer number (around 2000) they can churn through computations very quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmJ0hjO5D-ZM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\"\"\"\n",
        "CUDA is a parallel computing platform\n",
        "and application programming interface\n",
        "that allows software to use certain types\n",
        "of graphics processing units for general purpose processing\n",
        "\"\"\";\n",
        "#CUDA -> Recommend CME213 (C++)\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0kadMeJD-ZN"
      },
      "source": [
        "Installing the GPU drivers and the CUDA toolkit can be quite messy, so if you just want to experiment with GPUs and deep learning libraries, you can use [Google colaboratory](https://colab.research.google.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCtE0kLaD-ZO"
      },
      "outputs": [],
      "source": [
        "gpu = torch.device(\"cuda\")\n",
        "cpu = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESIE5J08D-ZS"
      },
      "outputs": [],
      "source": [
        "A = torch.rand(100,100)\n",
        "B = torch.rand(100,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6VNz5SzD-ZU"
      },
      "outputs": [],
      "source": [
        "A@B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXjtNNqtD-ZW"
      },
      "outputs": [],
      "source": [
        "A_gpu = A.to(gpu)\n",
        "B_gpu = B.to(gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krIHa3ErD-ZY"
      },
      "outputs": [],
      "source": [
        "A_gpu@B_gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sox7ng2OD-ZZ"
      },
      "outputs": [],
      "source": [
        "A@B_gpu #this won't work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5oi8M-GD-Zc"
      },
      "outputs": [],
      "source": [
        "C_gpu = A_gpu@B_gpu\n",
        "C = C_gpu.to(cpu)\n",
        "C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vICVTE1wD-Zq"
      },
      "source": [
        "## Speedup from GPU\n",
        "`%%timeit` is a Jupyter Notebook magic command that is used to measure the execution time of a Python code snippet. When you add `%%timeit` at the beginning of a cell in a Jupyter Notebook, it will run the code in the cell multiple times and measure the average execution time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "g4xQohqOMMMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4raRnuw1D-Zr"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "A = torch.rand(3000,3000)\n",
        "B = torch.rand(3000,3000)\n",
        "for i in range(5):\n",
        "    B=torch.mm(A,B)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch47eB6OD-Zt"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "A = torch.rand(3000,3000, device = gpu)\n",
        "B = torch.rand(3000,3000, device = gpu)\n",
        "for i in range(5):\n",
        "    B=torch.mm(A,B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDNQLaL6D-Zu"
      },
      "source": [
        "## Automatic Differentiation\n",
        "\n",
        "PyTorch uses dynamic computation graphs to compute the gradients of the parameters. Only certain parameters need gradients: i.e. the parameters we are wanting to update: weights and biases!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r6mfgjHD-Zv"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([2.0])\n",
        "w = torch.tensor([5.0], requires_grad = True)\n",
        "b = torch.tensor([2.0], requires_grad = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eezGUNqXD-Zy"
      },
      "outputs": [],
      "source": [
        "y = w*x + b #12 = 5*2 + 2\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Y_WzasD-Z0"
      },
      "source": [
        "Define an error for your function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r7oYMHFD-Z0"
      },
      "outputs": [],
      "source": [
        "loss = torch.norm( y - 13)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMhtVXAhD-Z5"
      },
      "source": [
        "Calling `x.backward()` on any tensor forces PyTorch to compute all the gradients of the tensors used to compute `x` which had the `requires_grad` flag set to `True`. The computed gradient will be stored in the `.grad` property of the tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PIU90uoD-Z5"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYbns6g4D-Z7"
      },
      "outputs": [],
      "source": [
        "w.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MYesARFD-Z9"
      },
      "outputs": [],
      "source": [
        "b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9GBX-JwVykN"
      },
      "source": [
        "We now can use the gradients of $w$ and $b$ to update their values (i.e. gradient descent). A helpful trick we can use here is telling PyTorch to skip the gradient calculations, which can help to reduce the memory usage and speed up computations. In this case, we don't want to update the gradients of $w$ and $b$ based on this parameter update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDhPOJHRD-aA"
      },
      "outputs": [],
      "source": [
        "# It will reduce memory consumption for computations\n",
        "# that would otherwise have requires_grad=True\n",
        "with torch.no_grad(): #when we update w and b, do not update gradients\n",
        "    w -= 0.01 * w.grad\n",
        "    b -= 0.3 * b.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TlUBDHaD-aC"
      },
      "outputs": [],
      "source": [
        "w,b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj85MTj2D-aF"
      },
      "outputs": [],
      "source": [
        "w.grad, b.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3UbDFA7D-aJ"
      },
      "outputs": [],
      "source": [
        "# the gradients will accumulate and lead to incorrect updates and slower convergence.\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "\n",
        "w.grad, b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After our update to our parameters, we can do another run through the network (forward propagation)."
      ],
      "metadata": {
        "id": "FENX09IOR7LK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNgA5bqxD-aL"
      },
      "outputs": [],
      "source": [
        "y = w*x + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhEnrtmQD-aN"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, our prediction for $y$ has improved from 12 -> 12.34, meaning our $w$ and $b$ approximations have improved. We can calculate the loss again to see that it has improved, and again use backpropagation to update the gradients of $w$ and $b$."
      ],
      "metadata": {
        "id": "gMfXWragSGWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJYE5aRpD-aO"
      },
      "outputs": [],
      "source": [
        "loss = torch.norm( y - 13)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBLYqopxD-aQ"
      },
      "outputs": [],
      "source": [
        "loss.backward()\n",
        "w.grad, b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNbj9oDlD-aS"
      },
      "source": [
        "### Making it more compact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlQ60VuqD-aS"
      },
      "outputs": [],
      "source": [
        "def model_fn(x,w,b):\n",
        "    return w*x + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zZHeiwJD-aV"
      },
      "outputs": [],
      "source": [
        "def loss_fn(y,yt):\n",
        "    return torch.norm(y-yt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7drmZAzD-aX"
      },
      "outputs": [],
      "source": [
        "w = torch.tensor([5.0], requires_grad = True)\n",
        "b = torch.tensor([2.0], requires_grad = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSogMXf-D-aY"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([2.0])\n",
        "yt = torch.tensor([13.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77BNsdU-D-aa"
      },
      "outputs": [],
      "source": [
        "y = model_fn(x,w,b)\n",
        "loss = loss_fn(y,yt)\n",
        "loss.backward()\n",
        "with torch.no_grad():\n",
        "    w -= 0.05 * w.grad\n",
        "    b -= 0.05 * b.grad\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "\n",
        "print( f\" w = {w}\\n b = {b}\\n y = {y}\\n loss = {loss}\")\n",
        "#note that 'loss' indicates the loss for the previous values\n",
        "for epoch in range(1,5):\n",
        "    # running again\n",
        "    print(15*'-')\n",
        "    y = model_fn(x,w,b)\n",
        "    loss = loss_fn(y,yt)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= 0.05 * w.grad\n",
        "        b -= 0.05 * b.grad\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    print( f\"epoch = {epoch}\\n w = {w} \\n b = {b}\\n y = {y}\\n loss = {loss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kRdaQe6D-ab"
      },
      "source": [
        "### Slightly more complicated problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNdBCLA1yTn4"
      },
      "source": [
        "1. Forward propagation to get predicted yhat\n",
        "2. Get the loss by loss_fun(y, yhat)\n",
        "3. Backward propagation to get the gradient\n",
        "4. Update parameters\n",
        "5. Repeat 1 ~ 4 until convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Alq94bPxD-ac"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HdYDtWDD-ae"
      },
      "outputs": [],
      "source": [
        "def model_fn(x,w,b):\n",
        "    return w@x + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6Cvos0BD-af"
      },
      "outputs": [],
      "source": [
        "def loss_fn(y,yt):\n",
        "    return torch.norm(y-yt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vCjbW7HD-ah"
      },
      "outputs": [],
      "source": [
        "w = torch.rand((5,5), requires_grad = True)\n",
        "b = torch.ones((5,1), requires_grad = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsITRCClD-ai"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(5,100)\n",
        "yt = torch.randn(1,100)\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFs5CphAD-al"
      },
      "outputs": [],
      "source": [
        "for i in range(50):\n",
        "  # 1. Forward\n",
        "  y = model_fn(x,w,b)\n",
        "  # 2. Get loss\n",
        "  loss = loss_fn(y,yt)\n",
        "  # 3. backward\n",
        "  loss.backward()\n",
        "  # 4. Update\n",
        "  with torch.no_grad():\n",
        "      w -= 0.05 * w.grad\n",
        "      b -= 0.05 * b.grad\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()\n",
        "\n",
        "  losses+=[loss.item()]\n",
        "  print( f\"loss = {loss}\")\n",
        "  plt.plot(losses);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t7iYj2eD-an"
      },
      "source": [
        "## Using Library functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xg-kywmD-an"
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Sequential(\n",
        "    # create layer of sequence\n",
        "    # F(WX + B)\n",
        "    # First layer: ReLU(WX + B)\n",
        "      # ReLU is important because it is doing some non-linear tranformation\n",
        "    torch.nn.Linear(5, 5),\n",
        "    torch.nn.ReLU(),\n",
        "\n",
        "    # Output layer\n",
        "    torch.nn.Linear(5, 1),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OocRUb9D-ap"
      },
      "outputs": [],
      "source": [
        "list(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgBOzYv5D-aq"
      },
      "outputs": [],
      "source": [
        "#mean square loss → MSE = (1/n) * sum((y_pred - y_true)^2)\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bdGByKKD-as"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(100,5)\n",
        "yt = torch.randn(100,1)\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KupJGFEFD-at"
      },
      "source": [
        "Using the optim package to get an optimizer (we hard-coded standard gradient descent to update $w$ and $b$, here it will update itself based on whatever optimizer we choose).\n",
        "\n",
        "Common optimizers:\n",
        "\n",
        "*   Adam\n",
        "*   SGD (Stochastic Gradient Descent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ac8_-reD-au"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1T-hdaSYBU7"
      },
      "source": [
        "`optimizer.step()` is used to update the model parameters based on the gradients computed during backpropagation. During training, the optimizer computes the gradients of the loss function with respect to the model parameters\n",
        "  \n",
        "`optimizer.zero_grad()` is used to set the gradients of all the model parameters to zero before computing the gradients for the next batch of data. If we don't zero out the gradients before computing the gradients for the next batch, the gradients will accumulate, leading to incorrect updates and slower convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtdxXUcRD-az"
      },
      "outputs": [],
      "source": [
        "for i in range(1000):\n",
        "    # feed data through model to get prediction\n",
        "    y = model(x)\n",
        "\n",
        "    # calculate loss (compare prediction to true value)\n",
        "    loss = loss_fn(y,yt)\n",
        "\n",
        "    # calculate gradients of parameters\n",
        "    loss.backward()\n",
        "\n",
        "    #make the parameter to take a step -> update the parameter\n",
        "    optimizer.step()\n",
        "\n",
        "    # clear gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    losses+=[loss.item()]\n",
        "    print( f\"loss = {loss}\")\n",
        "plt.plot(losses);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRlkcpT0D-a1"
      },
      "source": [
        "## MNIST Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4GfdMRhD-a1"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyQ4gP46D-a3"
      },
      "outputs": [],
      "source": [
        "data = MNIST(\".\",download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdfX2XnGD-a4"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7fwSvMkD-a5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "img,y = data[np.random.randint(1,60000)]\n",
        "print(y)\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSkUnhyaWfGG"
      },
      "outputs": [],
      "source": [
        "img,y = data[2]\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3giZTnRiD-a-"
      },
      "outputs": [],
      "source": [
        "print(data.data[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsZHdAMlD-a_"
      },
      "outputs": [],
      "source": [
        "print(data.targets[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROmpaSMVD-bA"
      },
      "source": [
        "### MNIST Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHhKarpfD-bB"
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Sequential( # 28*28 = 784\n",
        "    torch.nn.Linear(784,  100),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(100, 100),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(100, 10),\n",
        "    torch.nn.LogSoftmax(dim=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8Q59QCED-bC"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehneQZatD-bG"
      },
      "outputs": [],
      "source": [
        "sample = np.random.choice(range(len(data.data)),1000)\n",
        "x = data.data[sample].reshape(1000,-1).float()/255\n",
        "yt = data.targets[sample]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucZS9UPAD-bJ"
      },
      "outputs": [],
      "source": [
        "x.shape,yt.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdOxW-tYD-bK"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMtNhgaED-bL"
      },
      "outputs": [],
      "source": [
        "for i in range(100):\n",
        "    # get data and true labels\n",
        "    sample = np.random.choice(range(len(data.data)),1000)\n",
        "    x = data.data[sample].reshape(1000,-1).float()/255\n",
        "    yt = data.targets[sample]\n",
        "\n",
        "    # get prediction\n",
        "    y = model(x)\n",
        "\n",
        "    # input (Tensor) – Predicted unnormalized logits\n",
        "    # target (Tensor) – Ground truth class indices or class probabilities\n",
        "    loss = loss_fn(y,yt)\n",
        "\n",
        "    # compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters and clear previous gradients\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    losses+=[loss.item()]\n",
        "    #print( f\"loss = {loss}\")\n",
        "plt.plot(losses);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BAXeaKHD-bM"
      },
      "outputs": [],
      "source": [
        "x_test = data.data[-1000:].reshape(1000,-1).float()/255\n",
        "y_test = data.targets[-1000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VXFOBCjD-bO"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    y_pred = model(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms69eUDYYMGj"
      },
      "outputs": [],
      "source": [
        "y_test[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oG-bennMI3Y"
      },
      "outputs": [],
      "source": [
        "y_pred.argmax(dim=1)[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt0c-JhpD-bP"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy = \", (y_pred.argmax(dim=1) == y_test).sum().float().item()/1000.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY6m7c0qD-bQ"
      },
      "source": [
        "## Course Conclusion\n",
        "\n",
        "1. By now you should have a sufficient introduction to the various ways one can use python for scientific computing. The best way to learn more is to start using python for whatever project you are working on. Only practice will make you comfortable with using python.   \n",
        "\n",
        "Recommended Project Source: kaggle  \n",
        "\n",
        "Recommended ML/DL Courses: CS229, 230, 231N, 224 series, 238, 246  \n",
        "\n",
        "2. I appreciate your time to submit the course feedback, which means a lot to me and improvement for this course in the future  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pDnSshCXD-ZL",
        "2JDTirUfD-Zf",
        "vICVTE1wD-Zq",
        "wDNQLaL6D-Zu",
        "hNbj9oDlD-aS",
        "3kRdaQe6D-ab",
        "5t7iYj2eD-an",
        "tRlkcpT0D-a1",
        "ROmpaSMVD-bA",
        "VY6m7c0qD-bQ"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "PyTorch-GPU",
      "language": "python",
      "name": "pyt-gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}