{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VNIH93hD-Ya"
      },
      "source": [
        "# Lecture 8 - PyTorch\n",
        "\n",
        "This will be the final lecture, today we will first have a brief introduction of deep learning, then we will look at some basics of using PyTorch to implement some simple models in deep learning.\n",
        "\n",
        "1. Homework due today"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJYsdl4IbsO6"
      },
      "source": [
        "# Basic Section (Start)\n",
        "What is Neural Network:\n",
        "1. [Recommended youtube video with great visual helpers](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)\n",
        "2.  Slides on course site\n",
        "# Basic Section (End)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-fCZYu2D-Yc"
      },
      "source": [
        "## Deep Learning Libraries\n",
        "\n",
        "There are many deep learning libraries available, the most common ones for Python are\n",
        "\n",
        "- TensorFlow, Keras\n",
        "- PyTorch\n",
        "\n",
        "Working with TensorFlow requires going into lot of details of the contruction of the computation graph, whereas Keras is a higher level interface for TensorFlow. Tensorflow is very popular in the industry and good for production code.\n",
        "\n",
        "PyTorch can be used as low level interface, but is much more user-friendly than TensorFlow, but it also has a higher level interface. PyTorch is more popular in the research community.\n",
        "\n",
        "In Stanford courses and projects, it is very common to use PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzUDYHi8D-Yd"
      },
      "source": [
        "## Main features that any deep learning library should provide\n",
        "\n",
        "No matter what library or language you use, the main features provided by a deep learning library are\n",
        "1. Use the GPU to speed up computation\n",
        "2. Ability to do automatic differentiation\n",
        "3. Useful library functions for common architectures and optimization algorithms\n",
        "\n",
        "### PyTorch\n",
        "We will look at all of the above in pytorch.\n",
        "The best way to think about pytorch is that its numpy + GPU + autograd.\n",
        "\n",
        "You can install it with\n",
        "\n",
        "```conda install pytorch```.\n",
        "\n",
        "Alternatively (and recommended), run this notebook in Google Colab-- it provides an environment with all of the PyTorch dependencies plus a GPU free of charge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "fCdvNHW0D-Ye"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpWzZewHD-Yi"
      },
      "source": [
        "The equivalent object to numpy arrays in PyTorch are called tensors, but they are just multidimensional arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "t78yenP1D-Yj",
        "outputId": "d58ccdcd-3f20-464c-e449-b8c92cc4eb8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "torch.tensor([2,3,4,5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Efg1UeizD-Ym",
        "outputId": "4fd0aa25-85fa-414f-8cd0-c23d2dbea387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "torch.zeros((5,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "1BlufhDpD-Yp",
        "outputId": "e814f6d3-d483-4065-92e5-1957d4a922fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "x = torch.ones((5,5))\n",
        "print(type(x))\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "acgFdW_4D-Yr",
        "outputId": "f0e111e7-47b0-4aed-9bfd-817b659e7a7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[7., 7., 7., 7., 7.],\n",
              "        [7., 7., 7., 7., 7.],\n",
              "        [7., 7., 7., 7., 7.],\n",
              "        [7., 7., 7., 7., 7.],\n",
              "        [7., 7., 7., 7., 7.]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "2*x + 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "NwCz7O1wD-Yu",
        "outputId": "78e686ba-2a06-47dc-c5b9-fe31324d120a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5098, -0.1428,  0.7089, -1.2880, -0.4879],\n",
              "        [-0.1388, -0.3422, -1.7130,  0.4208,  0.0902],\n",
              "        [-0.0070,  0.2284, -0.2772,  0.5980,  1.2274],\n",
              "        [-1.7381, -1.3370,  0.1945, -0.6324, -0.0259],\n",
              "        [-0.5987, -0.5542,  1.0880,  1.7805, -1.1394]])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "torch.randn(5,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "4fHiY5VKD-Yw",
        "outputId": "c1bc2e51-0b13-4f0c-fef0-6d7e5375ead9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7969, 0.0792, 0.8780, 0.8403, 0.5745, 0.7364, 0.4532, 0.5269, 0.6353,\n",
              "        0.0493, 0.0394, 0.3111, 0.3738, 0.5245, 0.2594, 0.4753, 0.0732, 0.3152,\n",
              "        0.5095, 0.2218, 0.2549, 0.9722, 0.5688, 0.9450, 0.1898])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "x = torch.rand(25)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "0_QKyI7hD-Yz",
        "outputId": "83d706b4-e183-4b60-931a-88813f15c1f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7969, 0.0792, 0.8780, 0.8403, 0.5745],\n",
              "        [0.7364, 0.4532, 0.5269, 0.6353, 0.0493],\n",
              "        [0.0394, 0.3111, 0.3738, 0.5245, 0.2594],\n",
              "        [0.4753, 0.0732, 0.3152, 0.5095, 0.2218],\n",
              "        [0.2549, 0.9722, 0.5688, 0.9450, 0.1898]])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "x=x.reshape(-1,5)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "WLKjs14-D-Y3",
        "outputId": "150655c2-5e6c-46e6-e04d-75d8cd5d0e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Kn9fwJoSD-Y5",
        "outputId": "fc803207-2d4a-4c86-fe44-9f9f785c0752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 1.]])\n",
            "tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n",
            "        1.0000])\n"
          ]
        }
      ],
      "source": [
        "print(torch.arange(10))\n",
        "print(torch.eye(5))\n",
        "print(torch.linspace(0,1,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cix7EXwSD-Y7"
      },
      "source": [
        "Some functions are a bit different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "fRrYVBDmjLpq",
        "outputId": "b9bd349d-55f5-4ee3-9803-eb40b958fdbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.57860653 0.12350709 0.41390229 0.17291162 0.76952849]\n",
            " [0.53944518 0.87342218 0.34375145 0.03383577 0.3693543 ]\n",
            " [0.03189781 0.77280011 0.14828126 0.29334995 0.10907554]\n",
            " [0.40576985 0.950174   0.58958208 0.07766999 0.91885133]\n",
            " [0.79989498 0.75777491 0.73824198 0.46470267 0.6413701 ]]\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.05845603],\n",
              "       [2.15980888],\n",
              "       [1.35540468],\n",
              "       [2.94204725],\n",
              "       [3.40198463]])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "A = np.random.rand(5,5)\n",
        "x = np.ones((5,1))\n",
        "print(A)\n",
        "print(x)\n",
        "\n",
        "A@x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "npGWD47jicvY",
        "outputId": "6ef17b26-8d58-45b3-bd2a-74dfd5cc71b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0654, 0.5832, 0.7834, 0.5172, 0.5806],\n",
            "        [0.2415, 0.9853, 0.0420, 0.7158, 0.8510],\n",
            "        [0.4330, 0.0905, 0.7468, 0.5740, 0.7172],\n",
            "        [0.9222, 0.2012, 0.9953, 0.7680, 0.5001],\n",
            "        [0.0733, 0.4108, 0.4638, 0.7694, 0.3507]])\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.5298],\n",
              "        [2.8356],\n",
              "        [2.5614],\n",
              "        [3.3867],\n",
              "        [2.0681]])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "A_ = torch.rand((5,5))\n",
        "x_ = torch.ones(5,1)\n",
        "print(A_)\n",
        "print(x_)\n",
        "A_@x_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oz9QB17QTCL"
      },
      "outputs": [],
      "source": [
        "# class Test:\n",
        "#   def __init__(self):\n",
        "#     self.__data = 'Great__'\n",
        "#     self.data = 'Great'\n",
        "# test = Test()\n",
        "# print(test.__data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.ones((5,1))\n",
        "print(x)"
      ],
      "metadata": {
        "id": "2Y1aKfSTKOFl",
        "outputId": "54b7b874-e49b-4031-ca21-c81947c2182d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "9BukQIL5D-Y8",
        "outputId": "65cb9745-8b18-4603-a4f7-e661dd0aa0ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ],
      "source": [
        "x_ = torch.ones(5,1)\n",
        "print(x_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpEwcH-JD-ZA"
      },
      "source": [
        "You can convert tensors to a numpy array that shares its memory with the pytorch tensor -> to use more library that are compatible to numpy but not pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "MquNPK71D-ZC",
        "outputId": "19f8e1aa-d405-44af-b01e-941738c2b140",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "x = torch.ones(5,5)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "aOeMqFrOD-ZE",
        "outputId": "349d9b88-2b7f-4a3c-e530-150f80d0361c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "xn = x.numpy()\n",
        "print(type(xn))\n",
        "xn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "bZlG0x9xD-ZH",
        "outputId": "0d60ac17-bf48-4b59-d98e-47ef0c8f1b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.,  1.,  1.,  1.,  1.],\n",
              "       [ 1.,  1.,  1.,  1.,  1.],\n",
              "       [ 1.,  1.,  1.,  1.,  1.],\n",
              "       [ 1.,  1.,  1.,  1.,  1.],\n",
              "       [ 1.,  1., 10.,  1.,  1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "# Changes in Numpy will cause changes in Tensor\n",
        "xn[4,2]=10\n",
        "xn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "sN6qJIsID-ZJ",
        "outputId": "89cd5c22-640e-41ab-b690-16d52f48c06d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
              "        [ 1.,  1.,  1.,  1.,  1.],\n",
              "        [ 1.,  1.,  1.,  1.,  1.],\n",
              "        [ 1.,  1.,  1.,  1.,  1.],\n",
              "        [ 1.,  1., 10.,  1.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDnSshCXD-ZL"
      },
      "source": [
        "### Using the GPU\n",
        "\n",
        "The GPU (Graphical Processing Unit) is a separate processing unit that is specialized to handle bulk computations required for rendering high quality graphics. It mainly consists of a large number of processor cores that are individually very slow, but because of their sheer number (around 2000) they can churn through computations very quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "pmJ0hjO5D-ZM",
        "outputId": "f4568d53-c5bb-4613-b661-946e353bf9e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "import torch\n",
        "\"\"\"\n",
        "CUDA is a parallel computing platform\n",
        "and application programming interface\n",
        "that allows software to use certain types\n",
        "of graphics processing units for general purpose processing\n",
        "\"\"\";\n",
        "#CUDA -> Recommend CME213 (C++)\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0kadMeJD-ZN"
      },
      "source": [
        "Installing the GPU drivers and the CUDA toolkit can be quite messy, so if you just want to experiment with GPUs and deep learning libraries, you can use [Google colaboratory](https://colab.research.google.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "fCtE0kLaD-ZO"
      },
      "outputs": [],
      "source": [
        "gpu = torch.device(\"cuda\")\n",
        "cpu = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "ESIE5J08D-ZS"
      },
      "outputs": [],
      "source": [
        "A = torch.rand(100,100)\n",
        "B = torch.rand(100,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "t6VNz5SzD-ZU",
        "outputId": "cfa7edc2-3de1-47a1-df8b-237b81dc13ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[26.3180, 26.2944, 23.6101,  ..., 26.1075, 24.1745, 24.6728],\n",
              "        [25.8878, 26.9902, 23.9919,  ..., 27.1890, 26.1619, 26.0173],\n",
              "        [22.8132, 24.1727, 22.9417,  ..., 25.9095, 22.9610, 23.9043],\n",
              "        ...,\n",
              "        [25.8806, 27.0250, 24.7991,  ..., 24.8536, 24.7872, 25.0519],\n",
              "        [25.0513, 24.8801, 24.9222,  ..., 24.6244, 23.6362, 24.1914],\n",
              "        [23.3239, 22.9218, 22.7339,  ..., 23.5111, 21.5087, 23.1358]])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "A@B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "vXjtNNqtD-ZW"
      },
      "outputs": [],
      "source": [
        "A_gpu = A.to(gpu)\n",
        "B_gpu = B.to(gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "krIHa3ErD-ZY",
        "outputId": "b6e92b19-2fb7-4efe-9056-a07290244a28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[26.3180, 26.2944, 23.6101,  ..., 26.1075, 24.1745, 24.6728],\n",
              "        [25.8878, 26.9902, 23.9919,  ..., 27.1890, 26.1619, 26.0173],\n",
              "        [22.8132, 24.1727, 22.9417,  ..., 25.9095, 22.9610, 23.9043],\n",
              "        ...,\n",
              "        [25.8806, 27.0250, 24.7991,  ..., 24.8536, 24.7872, 25.0519],\n",
              "        [25.0513, 24.8801, 24.9222,  ..., 24.6244, 23.6362, 24.1914],\n",
              "        [23.3239, 22.9218, 22.7339,  ..., 23.5111, 21.5087, 23.1358]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "A_gpu@B_gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "sox7ng2OD-ZZ",
        "outputId": "1f7d6841-e721-48f3-c0aa-6db599f99106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-2e3381049b9a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mB_gpu\u001b[0m \u001b[0;31m#this won't work!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
          ]
        }
      ],
      "source": [
        "A@B_gpu #this won't work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "j5oi8M-GD-Zc",
        "outputId": "3fe3005b-9632-4f98-b209-d73b1d46b75f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[26.3180, 26.2944, 23.6101,  ..., 26.1075, 24.1745, 24.6728],\n",
              "        [25.8878, 26.9902, 23.9919,  ..., 27.1890, 26.1619, 26.0173],\n",
              "        [22.8132, 24.1727, 22.9417,  ..., 25.9095, 22.9610, 23.9043],\n",
              "        ...,\n",
              "        [25.8806, 27.0250, 24.7991,  ..., 24.8536, 24.7872, 25.0519],\n",
              "        [25.0513, 24.8801, 24.9222,  ..., 24.6244, 23.6362, 24.1914],\n",
              "        [23.3239, 22.9218, 22.7339,  ..., 23.5111, 21.5087, 23.1358]])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "C_gpu = A_gpu@B_gpu\n",
        "C = C_gpu.to(cpu)\n",
        "C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vICVTE1wD-Zq"
      },
      "source": [
        "## Speedup from GPU\n",
        "`%%timeit` is a Jupyter Notebook magic command that is used to measure the execution time of a Python code snippet. When you add `%%timeit` at the beginning of a cell in a Jupyter Notebook, it will run the code in the cell multiple times and measure the average execution time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "4raRnuw1D-Zr",
        "outputId": "0dbc8147-4185-457c-c7ea-cf33f6901d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.61 s ± 271 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "A = torch.rand(3000,3000)\n",
        "B = torch.rand(3000,3000)\n",
        "for i in range(5):\n",
        "    B=torch.mm(A,B)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "Ch47eB6OD-Zt",
        "outputId": "fa9f202f-bd28-46b6-cef7-07461a01fc5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70 ms ± 1.89 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "A = torch.rand(3000,3000, device = gpu)\n",
        "B = torch.rand(3000,3000, device = gpu)\n",
        "for i in range(5):\n",
        "    B=torch.mm(A,B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDNQLaL6D-Zu"
      },
      "source": [
        "## Automatic Differentiation\n",
        "\n",
        "PyTorch uses dynamic computation graphs to compute the gradients of the parameters. Only certain parameters need gradients: i.e. the parameters we are wanting to update: weights and biases!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "1r6mfgjHD-Zv"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([2.0])\n",
        "w = torch.tensor([5.0], requires_grad = True)\n",
        "b = torch.tensor([2.0], requires_grad = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "eezGUNqXD-Zy",
        "outputId": "b978c1e0-7e2a-458c-c8f1-736d68715c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([12.], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "y = w*x + b #12 = 5*2 + 2\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Y_WzasD-Z0"
      },
      "source": [
        "Define an error for your function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "3r7oYMHFD-Z0",
        "outputId": "abc1c6a5-c8ec-43b0-aedf-c6f388d80c92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., grad_fn=<LinalgVectorNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "loss = torch.norm( y - 13)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMhtVXAhD-Z5"
      },
      "source": [
        "Calling `x.backward()` on any tensor forces PyTorch to compute all the gradients of the tensors used to compute `x` which had the `requires_grad` flag set to `True`. The computed gradient will be stored in the `.grad` property of the tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "5PIU90uoD-Z5"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "vYbns6g4D-Z7",
        "outputId": "3f888484-c923-455d-90f2-594ab550394c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "w.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "0MYesARFD-Z9",
        "outputId": "1b38a691-700f-4808-cb7d-6e0acc094fa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9GBX-JwVykN"
      },
      "source": [
        "We now can use the gradients of $w$ and $b$ to update their values (i.e. gradient descent). A helpful trick we can use here is telling PyTorch to skip the gradient calculations, which can help to reduce the memory usage and speed up computations. In this case, we don't want to update the gradients of $w$ and $b$ based on this parameter update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "XDhPOJHRD-aA"
      },
      "outputs": [],
      "source": [
        "# It will reduce memory consumption for computations\n",
        "# that would otherwise have requires_grad=True\n",
        "with torch.no_grad(): #when we update w and b, do not update gradients\n",
        "    w -= 0.01 * w.grad\n",
        "    b -= 0.3 * b.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "6TlUBDHaD-aC",
        "outputId": "1d2cece9-4e83-4246-8d8a-c26e84ff583d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([5.0200], requires_grad=True), tensor([2.3000], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "w,b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Lj85MTj2D-aF",
        "outputId": "1e70893d-26e2-4208-d1d2-5ffe702c0308",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-2.]), tensor([-1.]))"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "w.grad, b.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "J3UbDFA7D-aJ",
        "outputId": "b6c7743a-9123-4abc-cee8-6a8a491a56af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.]), tensor([0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "# the gradients will accumulate and lead to incorrect updates and slower convergence.\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "\n",
        "w.grad, b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After our update to our parameters, we can do another run through the network (forward propagation)."
      ],
      "metadata": {
        "id": "FENX09IOR7LK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "PNgA5bqxD-aL"
      },
      "outputs": [],
      "source": [
        "y = w*x + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "LhEnrtmQD-aN",
        "outputId": "3b388b72-1924-4700-e967-268a4b0c94f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([12.3400], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, our prediction for $y$ has improved from 12 -> 12.34, meaning our $w$ and $b$ approximations have improved. We can calculate the loss again to see that it has improved, and again use backpropagation to update the gradients of $w$ and $b$."
      ],
      "metadata": {
        "id": "gMfXWragSGWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "QJYE5aRpD-aO",
        "outputId": "e0cdb026-50b9-4276-9eb2-ddd9f2059111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6600, grad_fn=<LinalgVectorNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "loss = torch.norm( y - 13)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "BBLYqopxD-aQ",
        "outputId": "5c7fd9be-1cda-402d-edee-0f7fba50d807",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-2.]), tensor([-1.]))"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "loss.backward()\n",
        "w.grad, b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNbj9oDlD-aS"
      },
      "source": [
        "### Making it more compact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "xlQ60VuqD-aS"
      },
      "outputs": [],
      "source": [
        "def model_fn(x,w,b):\n",
        "    return w*x + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "9zZHeiwJD-aV"
      },
      "outputs": [],
      "source": [
        "def loss_fn(y,yt):\n",
        "    return torch.norm(y-yt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "q7drmZAzD-aX"
      },
      "outputs": [],
      "source": [
        "w = torch.tensor([5.0], requires_grad = True)\n",
        "b = torch.tensor([2.0], requires_grad = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "ZSogMXf-D-aY"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([2.0])\n",
        "yt = torch.tensor([13.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "77BNsdU-D-aa",
        "outputId": "b17c3902-9b1a-4bb9-9aee-6b04d2aed200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " w = tensor([5.1000], requires_grad=True)\n",
            " b = tensor([2.0500], requires_grad=True)\n",
            " y = tensor([12.], grad_fn=<AddBackward0>)\n",
            " loss = 1.0\n",
            "---------------\n",
            "epoch = 1\n",
            " w = tensor([5.2000], requires_grad=True) \n",
            " b = tensor([2.1000], requires_grad=True)\n",
            " y = tensor([12.2500], grad_fn=<AddBackward0>)\n",
            " loss = 0.75\n",
            "---------------\n",
            "epoch = 2\n",
            " w = tensor([5.3000], requires_grad=True) \n",
            " b = tensor([2.1500], requires_grad=True)\n",
            " y = tensor([12.5000], grad_fn=<AddBackward0>)\n",
            " loss = 0.5\n",
            "---------------\n",
            "epoch = 3\n",
            " w = tensor([5.4000], requires_grad=True) \n",
            " b = tensor([2.2000], requires_grad=True)\n",
            " y = tensor([12.7500], grad_fn=<AddBackward0>)\n",
            " loss = 0.2500009536743164\n",
            "---------------\n",
            "epoch = 4\n",
            " w = tensor([5.5000], requires_grad=True) \n",
            " b = tensor([2.2500], requires_grad=True)\n",
            " y = tensor([13.0000], grad_fn=<AddBackward0>)\n",
            " loss = 9.5367431640625e-07\n"
          ]
        }
      ],
      "source": [
        "y = model_fn(x,w,b)\n",
        "loss = loss_fn(y,yt)\n",
        "loss.backward()\n",
        "with torch.no_grad():\n",
        "    w -= 0.05 * w.grad\n",
        "    b -= 0.05 * b.grad\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "\n",
        "print( f\" w = {w}\\n b = {b}\\n y = {y}\\n loss = {loss}\")\n",
        "#note that 'loss' indicates the loss for the previous values\n",
        "for epoch in range(1,5):\n",
        "    # running again\n",
        "    print(15*'-')\n",
        "    y = model_fn(x,w,b)\n",
        "    loss = loss_fn(y,yt)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= 0.05 * w.grad\n",
        "        b -= 0.05 * b.grad\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    print( f\"epoch = {epoch}\\n w = {w} \\n b = {b}\\n y = {y}\\n loss = {loss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kRdaQe6D-ab"
      },
      "source": [
        "### Slightly more complicated problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNdBCLA1yTn4"
      },
      "source": [
        "1. Forward propagation to get predicted yhat\n",
        "2. Get the loss by loss_fun(y, yhat)\n",
        "3. Backward propagation to get the gradient\n",
        "4. Update parameters\n",
        "5. Repeat 1 ~ 4 until convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "Alq94bPxD-ac"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "0HdYDtWDD-ae"
      },
      "outputs": [],
      "source": [
        "def model_fn(x,w,b):\n",
        "    return w@x + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "v6Cvos0BD-af"
      },
      "outputs": [],
      "source": [
        "def loss_fn(y,yt):\n",
        "    return torch.norm(y-yt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "8vCjbW7HD-ah"
      },
      "outputs": [],
      "source": [
        "w = torch.rand((5,5), requires_grad = True)\n",
        "b = torch.ones((5,1), requires_grad = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "BsITRCClD-ai"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(5,100)\n",
        "yt = torch.randn(1,100)\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "NFs5CphAD-al",
        "outputId": "0992407a-e255-41b7-9efa-9b619a4fd486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss = 44.82464599609375\n",
            "loss = 40.931732177734375\n",
            "loss = 37.377925872802734\n",
            "loss = 34.212364196777344\n",
            "loss = 31.477388381958008\n",
            "loss = 29.198528289794922\n",
            "loss = 27.374691009521484\n",
            "loss = 25.973690032958984\n",
            "loss = 24.937116622924805\n",
            "loss = 24.192968368530273\n",
            "loss = 23.669788360595703\n",
            "loss = 23.306255340576172\n",
            "loss = 23.054750442504883\n",
            "loss = 22.880611419677734\n",
            "loss = 22.759584426879883\n",
            "loss = 22.675024032592773\n",
            "loss = 22.615598678588867\n",
            "loss = 22.573612213134766\n",
            "loss = 22.543792724609375\n",
            "loss = 22.52252197265625\n",
            "loss = 22.50728988647461\n",
            "loss = 22.496349334716797\n",
            "loss = 22.488468170166016\n",
            "loss = 22.482776641845703\n",
            "loss = 22.478662490844727\n",
            "loss = 22.475679397583008\n",
            "loss = 22.47351837158203\n",
            "loss = 22.471948623657227\n",
            "loss = 22.470806121826172\n",
            "loss = 22.469974517822266\n",
            "loss = 22.469369888305664\n",
            "loss = 22.468931198120117\n",
            "loss = 22.468610763549805\n",
            "loss = 22.468374252319336\n",
            "loss = 22.46820640563965\n",
            "loss = 22.468082427978516\n",
            "loss = 22.46799087524414\n",
            "loss = 22.467924118041992\n",
            "loss = 22.467876434326172\n",
            "loss = 22.46784210205078\n",
            "loss = 22.467815399169922\n",
            "loss = 22.467796325683594\n",
            "loss = 22.467784881591797\n",
            "loss = 22.4677734375\n",
            "loss = 22.467763900756836\n",
            "loss = 22.46776008605957\n",
            "loss = 22.467756271362305\n",
            "loss = 22.46775245666504\n",
            "loss = 22.467750549316406\n",
            "loss = 22.467748641967773\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for i in range(50):\n",
        "  # 1. Forward\n",
        "  y = model_fn(x,w,b)\n",
        "  # 2. Get loss\n",
        "  loss = loss_fn(y,yt)\n",
        "  # 3. backward\n",
        "  loss.backward()\n",
        "  # 4. Update\n",
        "  with torch.no_grad():\n",
        "      w -= 0.05 * w.grad\n",
        "      b -= 0.05 * b.grad\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()\n",
        "\n",
        "  losses+=[loss.item()]\n",
        "  print( f\"loss = {loss}\")\n",
        "  plt.plot(losses);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t7iYj2eD-an"
      },
      "source": [
        "## Using Library functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "6xg-kywmD-an"
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Sequential(\n",
        "    # create layer of sequence\n",
        "    # F(WX + B)\n",
        "    # First layer: ReLU(WX + B)\n",
        "      # ReLU is important because it is doing some non-linear tranformation\n",
        "    torch.nn.Linear(5, 5),\n",
        "    torch.nn.ReLU(),\n",
        "\n",
        "    # Output layer\n",
        "    torch.nn.Linear(5, 1),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "4OocRUb9D-ap",
        "outputId": "f49112bb-2449-4c87-a669-a5a1fccc7962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.1830, -0.2978,  0.0333, -0.2771,  0.2529],\n",
              "         [ 0.0281,  0.3951, -0.0656, -0.2695, -0.4000],\n",
              "         [-0.0229, -0.1013,  0.1985, -0.4058, -0.1500],\n",
              "         [ 0.2423, -0.1930,  0.4154,  0.3227,  0.1673],\n",
              "         [ 0.2084, -0.1922,  0.2175,  0.1592, -0.2300]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.1145,  0.0325,  0.1902,  0.2504, -0.1801], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.1076,  0.1497, -0.2267, -0.2153, -0.1310]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.4372], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "list(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "DgBOzYv5D-aq"
      },
      "outputs": [],
      "source": [
        "#mean square loss → MSE = (1/n) * sum((y_pred - y_true)^2)\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "_bdGByKKD-as"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(100,5)\n",
        "yt = torch.randn(100,1)\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KupJGFEFD-at"
      },
      "source": [
        "Using the optim package to get an optimizer (we hard-coded standard gradient descent to update $w$ and $b$, here it will update itself based on whatever optimizer we choose).\n",
        "\n",
        "Common optimizers:\n",
        "\n",
        "*   Adam\n",
        "*   SGD (Stochastic Gradient Descent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "0Ac8_-reD-au"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1T-hdaSYBU7"
      },
      "source": [
        "`optimizer.step()` is used to update the model parameters based on the gradients computed during backpropagation. During training, the optimizer computes the gradients of the loss function with respect to the model parameters\n",
        "  \n",
        "`optimizer.zero_grad()` is used to set the gradients of all the model parameters to zero before computing the gradients for the next batch of data. If we don't zero out the gradients before computing the gradients for the next batch, the gradients will accumulate, leading to incorrect updates and slower convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "xtdxXUcRD-az",
        "outputId": "00e05e42-a49a-4507-a0cd-9dce458f1e1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss = 122.2519760131836\n",
            "loss = 112.2354507446289\n",
            "loss = 104.7008056640625\n",
            "loss = 99.31035614013672\n",
            "loss = 95.6781997680664\n",
            "loss = 93.50662994384766\n",
            "loss = 92.48434448242188\n",
            "loss = 92.26673126220703\n",
            "loss = 92.50293731689453\n",
            "loss = 92.8240966796875\n",
            "loss = 92.94263458251953\n",
            "loss = 92.74740600585938\n",
            "loss = 92.25298309326172\n",
            "loss = 91.52928924560547\n",
            "loss = 90.6669921875\n",
            "loss = 89.83712768554688\n",
            "loss = 89.16110229492188\n",
            "loss = 88.71623992919922\n",
            "loss = 88.42442321777344\n",
            "loss = 88.27655792236328\n",
            "loss = 88.22108459472656\n",
            "loss = 88.20304870605469\n",
            "loss = 88.16655731201172\n",
            "loss = 88.09562683105469\n",
            "loss = 88.01084899902344\n",
            "loss = 87.91629791259766\n",
            "loss = 87.79032897949219\n",
            "loss = 87.65595245361328\n",
            "loss = 87.47920989990234\n",
            "loss = 87.27147674560547\n",
            "loss = 87.07659149169922\n",
            "loss = 86.92936706542969\n",
            "loss = 86.8697280883789\n",
            "loss = 86.82525634765625\n",
            "loss = 86.76396179199219\n",
            "loss = 86.6867904663086\n",
            "loss = 86.5972900390625\n",
            "loss = 86.46392822265625\n",
            "loss = 86.28169250488281\n",
            "loss = 86.06932067871094\n",
            "loss = 85.78999328613281\n",
            "loss = 85.48159790039062\n",
            "loss = 85.17057037353516\n",
            "loss = 84.8770751953125\n",
            "loss = 84.57791137695312\n",
            "loss = 84.3089828491211\n",
            "loss = 84.09504699707031\n",
            "loss = 83.85789489746094\n",
            "loss = 83.6463394165039\n",
            "loss = 83.38402557373047\n",
            "loss = 83.13958740234375\n",
            "loss = 82.8455810546875\n",
            "loss = 82.48468780517578\n",
            "loss = 82.08143615722656\n",
            "loss = 81.81285858154297\n",
            "loss = 81.57071685791016\n",
            "loss = 81.2635269165039\n",
            "loss = 80.92994689941406\n",
            "loss = 80.59054565429688\n",
            "loss = 80.29517364501953\n",
            "loss = 79.96783447265625\n",
            "loss = 79.59205627441406\n",
            "loss = 79.17381286621094\n",
            "loss = 78.75125122070312\n",
            "loss = 78.33251953125\n",
            "loss = 77.94683074951172\n",
            "loss = 77.50470733642578\n",
            "loss = 77.08503723144531\n",
            "loss = 76.73418426513672\n",
            "loss = 76.42808532714844\n",
            "loss = 76.19135284423828\n",
            "loss = 75.78684997558594\n",
            "loss = 75.45360565185547\n",
            "loss = 75.13894653320312\n",
            "loss = 74.89791870117188\n",
            "loss = 74.4114990234375\n",
            "loss = 74.10425567626953\n",
            "loss = 73.67550659179688\n",
            "loss = 73.25984191894531\n",
            "loss = 72.88568878173828\n",
            "loss = 72.56134033203125\n",
            "loss = 72.21035766601562\n",
            "loss = 72.02995300292969\n",
            "loss = 71.76705169677734\n",
            "loss = 71.469482421875\n",
            "loss = 71.18305969238281\n",
            "loss = 70.83384704589844\n",
            "loss = 70.57032775878906\n",
            "loss = 70.3533935546875\n",
            "loss = 70.14396667480469\n",
            "loss = 69.88888549804688\n",
            "loss = 69.70854949951172\n",
            "loss = 69.48373413085938\n",
            "loss = 69.3408203125\n",
            "loss = 69.18424224853516\n",
            "loss = 68.99134063720703\n",
            "loss = 68.8001708984375\n",
            "loss = 68.70433044433594\n",
            "loss = 68.61605834960938\n",
            "loss = 68.51036071777344\n",
            "loss = 68.41065216064453\n",
            "loss = 68.30049133300781\n",
            "loss = 68.19557189941406\n",
            "loss = 68.09455871582031\n",
            "loss = 67.8941650390625\n",
            "loss = 67.7436294555664\n",
            "loss = 67.60713958740234\n",
            "loss = 67.45489501953125\n",
            "loss = 67.33373260498047\n",
            "loss = 67.12639617919922\n",
            "loss = 67.02143859863281\n",
            "loss = 66.90863800048828\n",
            "loss = 66.78494262695312\n",
            "loss = 66.88671875\n",
            "loss = 66.80900573730469\n",
            "loss = 66.6804428100586\n",
            "loss = 66.62371063232422\n",
            "loss = 66.57431030273438\n",
            "loss = 66.47376251220703\n",
            "loss = 66.45344543457031\n",
            "loss = 66.32372283935547\n",
            "loss = 66.3109130859375\n",
            "loss = 66.20132446289062\n",
            "loss = 66.17610168457031\n",
            "loss = 66.07637023925781\n",
            "loss = 65.96784973144531\n",
            "loss = 65.86027526855469\n",
            "loss = 65.79592895507812\n",
            "loss = 65.6936264038086\n",
            "loss = 65.60692596435547\n",
            "loss = 65.58396911621094\n",
            "loss = 65.5636215209961\n",
            "loss = 65.57684326171875\n",
            "loss = 65.60224151611328\n",
            "loss = 65.56134796142578\n",
            "loss = 65.53302764892578\n",
            "loss = 65.4789047241211\n",
            "loss = 65.54905700683594\n",
            "loss = 65.51836395263672\n",
            "loss = 65.41726684570312\n",
            "loss = 65.42724609375\n",
            "loss = 65.4249038696289\n",
            "loss = 65.43382263183594\n",
            "loss = 65.37606811523438\n",
            "loss = 65.29232788085938\n",
            "loss = 65.295654296875\n",
            "loss = 65.32009887695312\n",
            "loss = 65.34414672851562\n",
            "loss = 65.3421630859375\n",
            "loss = 65.32828521728516\n",
            "loss = 65.29502868652344\n",
            "loss = 65.24502563476562\n",
            "loss = 65.24031066894531\n",
            "loss = 65.24369812011719\n",
            "loss = 65.26822662353516\n",
            "loss = 65.23914337158203\n",
            "loss = 65.24300384521484\n",
            "loss = 65.1974868774414\n",
            "loss = 65.22584533691406\n",
            "loss = 65.23239135742188\n",
            "loss = 65.23265075683594\n",
            "loss = 65.2214584350586\n",
            "loss = 65.22502899169922\n",
            "loss = 65.2080078125\n",
            "loss = 65.1949462890625\n",
            "loss = 65.19789123535156\n",
            "loss = 65.1946792602539\n",
            "loss = 65.1900634765625\n",
            "loss = 65.19719696044922\n",
            "loss = 65.1723403930664\n",
            "loss = 65.17987060546875\n",
            "loss = 65.20543670654297\n",
            "loss = 65.19879150390625\n",
            "loss = 65.205322265625\n",
            "loss = 65.19664001464844\n",
            "loss = 65.18795776367188\n",
            "loss = 65.19152069091797\n",
            "loss = 65.17591857910156\n",
            "loss = 65.17398834228516\n",
            "loss = 65.18002319335938\n",
            "loss = 65.16309356689453\n",
            "loss = 65.16154479980469\n",
            "loss = 65.16947937011719\n",
            "loss = 65.17855072021484\n",
            "loss = 65.18362426757812\n",
            "loss = 65.18638610839844\n",
            "loss = 65.17361450195312\n",
            "loss = 65.16661834716797\n",
            "loss = 65.17053985595703\n",
            "loss = 65.17747497558594\n",
            "loss = 65.16136932373047\n",
            "loss = 65.16389465332031\n",
            "loss = 65.16812896728516\n",
            "loss = 65.16756439208984\n",
            "loss = 65.18486785888672\n",
            "loss = 65.20726776123047\n",
            "loss = 65.1968002319336\n",
            "loss = 65.18097686767578\n",
            "loss = 65.19822692871094\n",
            "loss = 65.18328857421875\n",
            "loss = 65.20101928710938\n",
            "loss = 65.20915222167969\n",
            "loss = 65.1737060546875\n",
            "loss = 65.17881774902344\n",
            "loss = 65.19685363769531\n",
            "loss = 65.17672729492188\n",
            "loss = 65.15353393554688\n",
            "loss = 65.15711975097656\n",
            "loss = 65.16657257080078\n",
            "loss = 65.16931915283203\n",
            "loss = 65.18724822998047\n",
            "loss = 65.19873046875\n",
            "loss = 65.1879653930664\n",
            "loss = 65.17539978027344\n",
            "loss = 65.17570495605469\n",
            "loss = 65.17395782470703\n",
            "loss = 65.1906509399414\n",
            "loss = 65.18590545654297\n",
            "loss = 65.17084503173828\n",
            "loss = 65.1661605834961\n",
            "loss = 65.16321563720703\n",
            "loss = 65.15927124023438\n",
            "loss = 65.15544128417969\n",
            "loss = 65.16299438476562\n",
            "loss = 65.16044616699219\n",
            "loss = 65.17304229736328\n",
            "loss = 65.1793212890625\n",
            "loss = 65.16839599609375\n",
            "loss = 65.16776275634766\n",
            "loss = 65.17826080322266\n",
            "loss = 65.1707534790039\n",
            "loss = 65.1759262084961\n",
            "loss = 65.18641662597656\n",
            "loss = 65.16305541992188\n",
            "loss = 65.15611267089844\n",
            "loss = 65.16189575195312\n",
            "loss = 65.16595458984375\n",
            "loss = 65.16635131835938\n",
            "loss = 65.17041778564453\n",
            "loss = 65.14319610595703\n",
            "loss = 65.18258666992188\n",
            "loss = 65.19671630859375\n",
            "loss = 65.1732406616211\n",
            "loss = 65.19996643066406\n",
            "loss = 65.20787811279297\n",
            "loss = 65.2188720703125\n",
            "loss = 65.20087432861328\n",
            "loss = 65.1669692993164\n",
            "loss = 65.18751525878906\n",
            "loss = 65.18854522705078\n",
            "loss = 65.16629028320312\n",
            "loss = 65.17203521728516\n",
            "loss = 65.18305969238281\n",
            "loss = 65.1734848022461\n",
            "loss = 65.16958618164062\n",
            "loss = 65.16407012939453\n",
            "loss = 65.19739532470703\n",
            "loss = 65.18798828125\n",
            "loss = 65.18357849121094\n",
            "loss = 65.21119689941406\n",
            "loss = 65.18968963623047\n",
            "loss = 65.19364166259766\n",
            "loss = 65.20781707763672\n",
            "loss = 65.19905853271484\n",
            "loss = 65.1655044555664\n",
            "loss = 65.17767333984375\n",
            "loss = 65.20618438720703\n",
            "loss = 65.1878890991211\n",
            "loss = 65.16950988769531\n",
            "loss = 65.23080444335938\n",
            "loss = 65.25618743896484\n",
            "loss = 65.22136688232422\n",
            "loss = 65.22952270507812\n",
            "loss = 65.23065185546875\n",
            "loss = 65.18578338623047\n",
            "loss = 65.23416137695312\n",
            "loss = 65.23110961914062\n",
            "loss = 65.19355010986328\n",
            "loss = 65.19943237304688\n",
            "loss = 65.2111587524414\n",
            "loss = 65.22322082519531\n",
            "loss = 65.17794036865234\n",
            "loss = 65.19429016113281\n",
            "loss = 65.20230865478516\n",
            "loss = 65.19117736816406\n",
            "loss = 65.16481018066406\n",
            "loss = 65.1800308227539\n",
            "loss = 65.19999694824219\n",
            "loss = 65.17695617675781\n",
            "loss = 65.17768859863281\n",
            "loss = 65.19950103759766\n",
            "loss = 65.19450378417969\n",
            "loss = 65.19264221191406\n",
            "loss = 65.19810485839844\n",
            "loss = 65.17123413085938\n",
            "loss = 65.16961669921875\n",
            "loss = 65.1883773803711\n",
            "loss = 65.1802978515625\n",
            "loss = 65.16622161865234\n",
            "loss = 65.16471099853516\n",
            "loss = 65.17904663085938\n",
            "loss = 65.1644515991211\n",
            "loss = 65.15248107910156\n",
            "loss = 65.15357208251953\n",
            "loss = 65.16419219970703\n",
            "loss = 65.17261505126953\n",
            "loss = 65.15680694580078\n",
            "loss = 65.15176391601562\n",
            "loss = 65.16722106933594\n",
            "loss = 65.168212890625\n",
            "loss = 65.16989135742188\n",
            "loss = 65.15979766845703\n",
            "loss = 65.15234375\n",
            "loss = 65.15062713623047\n",
            "loss = 65.16539001464844\n",
            "loss = 65.171630859375\n",
            "loss = 65.1539535522461\n",
            "loss = 65.18138885498047\n",
            "loss = 65.19476318359375\n",
            "loss = 65.15898895263672\n",
            "loss = 65.16398620605469\n",
            "loss = 65.17711639404297\n",
            "loss = 65.17230224609375\n",
            "loss = 65.15565490722656\n",
            "loss = 65.16168975830078\n",
            "loss = 65.15782928466797\n",
            "loss = 65.15601348876953\n",
            "loss = 65.16542053222656\n",
            "loss = 65.19094848632812\n",
            "loss = 65.19325256347656\n",
            "loss = 65.1706771850586\n",
            "loss = 65.16912078857422\n",
            "loss = 65.18228912353516\n",
            "loss = 65.18365478515625\n",
            "loss = 65.170654296875\n",
            "loss = 65.16947937011719\n",
            "loss = 65.1985092163086\n",
            "loss = 65.18240356445312\n",
            "loss = 65.16773223876953\n",
            "loss = 65.15950012207031\n",
            "loss = 65.17744445800781\n",
            "loss = 65.18869018554688\n",
            "loss = 65.14537811279297\n",
            "loss = 65.17935180664062\n",
            "loss = 65.20514678955078\n",
            "loss = 65.20341491699219\n",
            "loss = 65.18264770507812\n",
            "loss = 65.16938018798828\n",
            "loss = 65.19044494628906\n",
            "loss = 65.23619842529297\n",
            "loss = 65.1968002319336\n",
            "loss = 65.18670654296875\n",
            "loss = 65.21665954589844\n",
            "loss = 65.24864196777344\n",
            "loss = 65.23988342285156\n",
            "loss = 65.19791412353516\n",
            "loss = 65.18348693847656\n",
            "loss = 65.20826721191406\n",
            "loss = 65.21412658691406\n",
            "loss = 65.17083740234375\n",
            "loss = 65.18541717529297\n",
            "loss = 65.20914459228516\n",
            "loss = 65.21931457519531\n",
            "loss = 65.2238540649414\n",
            "loss = 65.19351959228516\n",
            "loss = 65.21420288085938\n",
            "loss = 65.2077407836914\n",
            "loss = 65.20993041992188\n",
            "loss = 65.17529296875\n",
            "loss = 65.1685791015625\n",
            "loss = 65.19306182861328\n",
            "loss = 65.20243835449219\n",
            "loss = 65.16978454589844\n",
            "loss = 65.17176055908203\n",
            "loss = 65.18484497070312\n",
            "loss = 65.18212890625\n",
            "loss = 65.16757202148438\n",
            "loss = 65.1759033203125\n",
            "loss = 65.17901611328125\n",
            "loss = 65.17984771728516\n",
            "loss = 65.1752700805664\n",
            "loss = 65.15733337402344\n",
            "loss = 65.19316101074219\n",
            "loss = 65.19005584716797\n",
            "loss = 65.21636962890625\n",
            "loss = 65.19978332519531\n",
            "loss = 65.18987274169922\n",
            "loss = 65.18177032470703\n",
            "loss = 65.19013214111328\n",
            "loss = 65.17625427246094\n",
            "loss = 65.1646957397461\n",
            "loss = 65.17990112304688\n",
            "loss = 65.1800537109375\n",
            "loss = 65.16364288330078\n",
            "loss = 65.18868255615234\n",
            "loss = 65.17231750488281\n",
            "loss = 65.171630859375\n",
            "loss = 65.17720031738281\n",
            "loss = 65.15766906738281\n",
            "loss = 65.16537475585938\n",
            "loss = 65.15280151367188\n",
            "loss = 65.18449401855469\n",
            "loss = 65.17180633544922\n",
            "loss = 65.185546875\n",
            "loss = 65.18645477294922\n",
            "loss = 65.16014862060547\n",
            "loss = 65.1492691040039\n",
            "loss = 65.17280578613281\n",
            "loss = 65.16572570800781\n",
            "loss = 65.1550064086914\n",
            "loss = 65.16162872314453\n",
            "loss = 65.17363739013672\n",
            "loss = 65.157958984375\n",
            "loss = 65.15652465820312\n",
            "loss = 65.16983795166016\n",
            "loss = 65.16706085205078\n",
            "loss = 65.16039276123047\n",
            "loss = 65.15553283691406\n",
            "loss = 65.15713500976562\n",
            "loss = 65.16539001464844\n",
            "loss = 65.18106842041016\n",
            "loss = 65.1907730102539\n",
            "loss = 65.21476745605469\n",
            "loss = 65.19759368896484\n",
            "loss = 65.17609405517578\n",
            "loss = 65.18411254882812\n",
            "loss = 65.19820404052734\n",
            "loss = 65.1744384765625\n",
            "loss = 65.16177368164062\n",
            "loss = 65.18729400634766\n",
            "loss = 65.1829833984375\n",
            "loss = 65.20362854003906\n",
            "loss = 65.18744659423828\n",
            "loss = 65.1798095703125\n",
            "loss = 65.17745971679688\n",
            "loss = 65.21263122558594\n",
            "loss = 65.193115234375\n",
            "loss = 65.15534210205078\n",
            "loss = 65.189453125\n",
            "loss = 65.2369384765625\n",
            "loss = 65.24419403076172\n",
            "loss = 65.18274688720703\n",
            "loss = 65.19192504882812\n",
            "loss = 65.19737243652344\n",
            "loss = 65.19944763183594\n",
            "loss = 65.16629028320312\n",
            "loss = 65.17740631103516\n",
            "loss = 65.21499633789062\n",
            "loss = 65.19448852539062\n",
            "loss = 65.18695068359375\n",
            "loss = 65.17608642578125\n",
            "loss = 65.19349670410156\n",
            "loss = 65.16999053955078\n",
            "loss = 65.16361236572266\n",
            "loss = 65.16827392578125\n",
            "loss = 65.15911102294922\n",
            "loss = 65.1798095703125\n",
            "loss = 65.17772674560547\n",
            "loss = 65.18186950683594\n",
            "loss = 65.17123413085938\n",
            "loss = 65.16860961914062\n",
            "loss = 65.16781616210938\n",
            "loss = 65.17349243164062\n",
            "loss = 65.17391204833984\n",
            "loss = 65.18441009521484\n",
            "loss = 65.18704986572266\n",
            "loss = 65.16241455078125\n",
            "loss = 65.16876220703125\n",
            "loss = 65.16426086425781\n",
            "loss = 65.1854248046875\n",
            "loss = 65.17521667480469\n",
            "loss = 65.18157958984375\n",
            "loss = 65.17190551757812\n",
            "loss = 65.17395782470703\n",
            "loss = 65.18157196044922\n",
            "loss = 65.17546844482422\n",
            "loss = 65.18556213378906\n",
            "loss = 65.17098999023438\n",
            "loss = 65.17581176757812\n",
            "loss = 65.17279815673828\n",
            "loss = 65.16883087158203\n",
            "loss = 65.1579360961914\n",
            "loss = 65.16250610351562\n",
            "loss = 65.17994689941406\n",
            "loss = 65.17453002929688\n",
            "loss = 65.17341613769531\n",
            "loss = 65.15131378173828\n",
            "loss = 65.17029571533203\n",
            "loss = 65.17682647705078\n",
            "loss = 65.16496276855469\n",
            "loss = 65.16230773925781\n",
            "loss = 65.16609191894531\n",
            "loss = 65.17328643798828\n",
            "loss = 65.19033813476562\n",
            "loss = 65.18453979492188\n",
            "loss = 65.16970825195312\n",
            "loss = 65.17343139648438\n",
            "loss = 65.20026397705078\n",
            "loss = 65.19293212890625\n",
            "loss = 65.15999603271484\n",
            "loss = 65.15901184082031\n",
            "loss = 65.19461059570312\n",
            "loss = 65.18333435058594\n",
            "loss = 65.18181610107422\n",
            "loss = 65.1806640625\n",
            "loss = 65.17353057861328\n",
            "loss = 65.1779556274414\n",
            "loss = 65.19781494140625\n",
            "loss = 65.1711196899414\n",
            "loss = 65.17767333984375\n",
            "loss = 65.22261810302734\n",
            "loss = 65.22069549560547\n",
            "loss = 65.22294616699219\n",
            "loss = 65.19149780273438\n",
            "loss = 65.18592834472656\n",
            "loss = 65.19255065917969\n",
            "loss = 65.17854309082031\n",
            "loss = 65.16570281982422\n",
            "loss = 65.16232299804688\n",
            "loss = 65.19401550292969\n",
            "loss = 65.1943130493164\n",
            "loss = 65.18873596191406\n",
            "loss = 65.1634750366211\n",
            "loss = 65.18417358398438\n",
            "loss = 65.1976089477539\n",
            "loss = 65.18025207519531\n",
            "loss = 65.15841674804688\n",
            "loss = 65.17322540283203\n",
            "loss = 65.19962310791016\n",
            "loss = 65.20620727539062\n",
            "loss = 65.18473815917969\n",
            "loss = 65.17845153808594\n",
            "loss = 65.19464874267578\n",
            "loss = 65.19524383544922\n",
            "loss = 65.17486572265625\n",
            "loss = 65.15711975097656\n",
            "loss = 65.19388580322266\n",
            "loss = 65.2071533203125\n",
            "loss = 65.1788558959961\n",
            "loss = 65.17417907714844\n",
            "loss = 65.199951171875\n",
            "loss = 65.19725799560547\n",
            "loss = 65.17039489746094\n",
            "loss = 65.18315887451172\n",
            "loss = 65.19659423828125\n",
            "loss = 65.15605926513672\n",
            "loss = 65.1936264038086\n",
            "loss = 65.20828247070312\n",
            "loss = 65.23381042480469\n",
            "loss = 65.21319580078125\n",
            "loss = 65.20282745361328\n",
            "loss = 65.21112060546875\n",
            "loss = 65.23036193847656\n",
            "loss = 65.20206451416016\n",
            "loss = 65.19529724121094\n",
            "loss = 65.19822692871094\n",
            "loss = 65.195068359375\n",
            "loss = 65.181884765625\n",
            "loss = 65.17113494873047\n",
            "loss = 65.17151641845703\n",
            "loss = 65.17938232421875\n",
            "loss = 65.18409729003906\n",
            "loss = 65.20329284667969\n",
            "loss = 65.22177124023438\n",
            "loss = 65.25137329101562\n",
            "loss = 65.21343994140625\n",
            "loss = 65.19537353515625\n",
            "loss = 65.23222351074219\n",
            "loss = 65.24240112304688\n",
            "loss = 65.19918060302734\n",
            "loss = 65.19667053222656\n",
            "loss = 65.24148559570312\n",
            "loss = 65.2359390258789\n",
            "loss = 65.21063232421875\n",
            "loss = 65.1911849975586\n",
            "loss = 65.19290161132812\n",
            "loss = 65.25920104980469\n",
            "loss = 65.2323989868164\n",
            "loss = 65.22283935546875\n",
            "loss = 65.24186706542969\n",
            "loss = 65.25248718261719\n",
            "loss = 65.21269226074219\n",
            "loss = 65.17892456054688\n",
            "loss = 65.21330261230469\n",
            "loss = 65.20698547363281\n",
            "loss = 65.18498992919922\n",
            "loss = 65.18306732177734\n",
            "loss = 65.1980209350586\n",
            "loss = 65.17811584472656\n",
            "loss = 65.20216369628906\n",
            "loss = 65.1880874633789\n",
            "loss = 65.1775131225586\n",
            "loss = 65.177001953125\n",
            "loss = 65.17626190185547\n",
            "loss = 65.1678695678711\n",
            "loss = 65.19268035888672\n",
            "loss = 65.19585418701172\n",
            "loss = 65.1828842163086\n",
            "loss = 65.18900299072266\n",
            "loss = 65.17807006835938\n",
            "loss = 65.17228698730469\n",
            "loss = 65.17025756835938\n",
            "loss = 65.18771362304688\n",
            "loss = 65.17693328857422\n",
            "loss = 65.20929718017578\n",
            "loss = 65.18556213378906\n",
            "loss = 65.19879913330078\n",
            "loss = 65.19989776611328\n",
            "loss = 65.21913146972656\n",
            "loss = 65.20660400390625\n",
            "loss = 65.16889190673828\n",
            "loss = 65.18770599365234\n",
            "loss = 65.18763732910156\n",
            "loss = 65.17569732666016\n",
            "loss = 65.16183471679688\n",
            "loss = 65.16400909423828\n",
            "loss = 65.18644714355469\n",
            "loss = 65.16188049316406\n",
            "loss = 65.16645050048828\n",
            "loss = 65.17909240722656\n",
            "loss = 65.16085052490234\n",
            "loss = 65.18363189697266\n",
            "loss = 65.21644592285156\n",
            "loss = 65.20742797851562\n",
            "loss = 65.2001724243164\n",
            "loss = 65.19580841064453\n",
            "loss = 65.19930267333984\n",
            "loss = 65.17420959472656\n",
            "loss = 65.17118835449219\n",
            "loss = 65.1766128540039\n",
            "loss = 65.19256591796875\n",
            "loss = 65.18853759765625\n",
            "loss = 65.17323303222656\n",
            "loss = 65.16029357910156\n",
            "loss = 65.17991638183594\n",
            "loss = 65.1805191040039\n",
            "loss = 65.19440460205078\n",
            "loss = 65.19876098632812\n",
            "loss = 65.21005249023438\n",
            "loss = 65.20231628417969\n",
            "loss = 65.20767211914062\n",
            "loss = 65.21430969238281\n",
            "loss = 65.17094421386719\n",
            "loss = 65.18724822998047\n",
            "loss = 65.21221923828125\n",
            "loss = 65.21443939208984\n",
            "loss = 65.22241973876953\n",
            "loss = 65.20527648925781\n",
            "loss = 65.21259307861328\n",
            "loss = 65.20337677001953\n",
            "loss = 65.20042419433594\n",
            "loss = 65.22374725341797\n",
            "loss = 65.22174072265625\n",
            "loss = 65.23200988769531\n",
            "loss = 65.2145004272461\n",
            "loss = 65.20067596435547\n",
            "loss = 65.1961441040039\n",
            "loss = 65.18856811523438\n",
            "loss = 65.21015167236328\n",
            "loss = 65.21575927734375\n",
            "loss = 65.21818542480469\n",
            "loss = 65.19914245605469\n",
            "loss = 65.1968994140625\n",
            "loss = 65.18960571289062\n",
            "loss = 65.21685791015625\n",
            "loss = 65.17694091796875\n",
            "loss = 65.19627380371094\n",
            "loss = 65.21009826660156\n",
            "loss = 65.20106506347656\n",
            "loss = 65.18915557861328\n",
            "loss = 65.18600463867188\n",
            "loss = 65.17813873291016\n",
            "loss = 65.17179870605469\n",
            "loss = 65.1687240600586\n",
            "loss = 65.169189453125\n",
            "loss = 65.15996551513672\n",
            "loss = 65.17816925048828\n",
            "loss = 65.18046569824219\n",
            "loss = 65.19180297851562\n",
            "loss = 65.18291473388672\n",
            "loss = 65.1607666015625\n",
            "loss = 65.18504333496094\n",
            "loss = 65.176025390625\n",
            "loss = 65.19840240478516\n",
            "loss = 65.19125366210938\n",
            "loss = 65.17413330078125\n",
            "loss = 65.17559051513672\n",
            "loss = 65.17074584960938\n",
            "loss = 65.19738006591797\n",
            "loss = 65.16905212402344\n",
            "loss = 65.16967010498047\n",
            "loss = 65.18018341064453\n",
            "loss = 65.15489196777344\n",
            "loss = 65.16597747802734\n",
            "loss = 65.17059326171875\n",
            "loss = 65.17333221435547\n",
            "loss = 65.22175598144531\n",
            "loss = 65.22129821777344\n",
            "loss = 65.16646575927734\n",
            "loss = 65.16734313964844\n",
            "loss = 65.19781494140625\n",
            "loss = 65.19872283935547\n",
            "loss = 65.19091796875\n",
            "loss = 65.17858123779297\n",
            "loss = 65.16978454589844\n",
            "loss = 65.19084930419922\n",
            "loss = 65.18113708496094\n",
            "loss = 65.16845703125\n",
            "loss = 65.16490936279297\n",
            "loss = 65.16184997558594\n",
            "loss = 65.17518615722656\n",
            "loss = 65.16898345947266\n",
            "loss = 65.18815612792969\n",
            "loss = 65.18508911132812\n",
            "loss = 65.18128204345703\n",
            "loss = 65.17192077636719\n",
            "loss = 65.18389892578125\n",
            "loss = 65.1930160522461\n",
            "loss = 65.18080139160156\n",
            "loss = 65.162353515625\n",
            "loss = 65.19473266601562\n",
            "loss = 65.207763671875\n",
            "loss = 65.19012451171875\n",
            "loss = 65.173828125\n",
            "loss = 65.17189025878906\n",
            "loss = 65.16812896728516\n",
            "loss = 65.15846252441406\n",
            "loss = 65.15690612792969\n",
            "loss = 65.17008209228516\n",
            "loss = 65.17330169677734\n",
            "loss = 65.21429443359375\n",
            "loss = 65.2276840209961\n",
            "loss = 65.20156860351562\n",
            "loss = 65.18669128417969\n",
            "loss = 65.19244384765625\n",
            "loss = 65.20100402832031\n",
            "loss = 65.17884826660156\n",
            "loss = 65.17533874511719\n",
            "loss = 65.2083511352539\n",
            "loss = 65.20275115966797\n",
            "loss = 65.18087005615234\n",
            "loss = 65.16435241699219\n",
            "loss = 65.1678237915039\n",
            "loss = 65.2006607055664\n",
            "loss = 65.19444274902344\n",
            "loss = 65.16940307617188\n",
            "loss = 65.16769409179688\n",
            "loss = 65.17430114746094\n",
            "loss = 65.18721008300781\n",
            "loss = 65.18911743164062\n",
            "loss = 65.206298828125\n",
            "loss = 65.19561767578125\n",
            "loss = 65.17407989501953\n",
            "loss = 65.17642211914062\n",
            "loss = 65.16983795166016\n",
            "loss = 65.1853256225586\n",
            "loss = 65.1836929321289\n",
            "loss = 65.18539428710938\n",
            "loss = 65.17817687988281\n",
            "loss = 65.2042236328125\n",
            "loss = 65.1854248046875\n",
            "loss = 65.1563720703125\n",
            "loss = 65.1939697265625\n",
            "loss = 65.20460510253906\n",
            "loss = 65.21031188964844\n",
            "loss = 65.2260971069336\n",
            "loss = 65.23924255371094\n",
            "loss = 65.19920349121094\n",
            "loss = 65.18885040283203\n",
            "loss = 65.18083190917969\n",
            "loss = 65.19094848632812\n",
            "loss = 65.19087982177734\n",
            "loss = 65.1893310546875\n",
            "loss = 65.1994857788086\n",
            "loss = 65.18783569335938\n",
            "loss = 65.1700439453125\n",
            "loss = 65.16631317138672\n",
            "loss = 65.18114471435547\n",
            "loss = 65.19317626953125\n",
            "loss = 65.1838607788086\n",
            "loss = 65.17346954345703\n",
            "loss = 65.16095733642578\n",
            "loss = 65.19357299804688\n",
            "loss = 65.22392272949219\n",
            "loss = 65.20665740966797\n",
            "loss = 65.20668029785156\n",
            "loss = 65.19744873046875\n",
            "loss = 65.19422149658203\n",
            "loss = 65.19396209716797\n",
            "loss = 65.19047546386719\n",
            "loss = 65.18978881835938\n",
            "loss = 65.17791748046875\n",
            "loss = 65.17048645019531\n",
            "loss = 65.1822509765625\n",
            "loss = 65.18376159667969\n",
            "loss = 65.17906188964844\n",
            "loss = 65.1568374633789\n",
            "loss = 65.1729736328125\n",
            "loss = 65.16081237792969\n",
            "loss = 65.1651840209961\n",
            "loss = 65.15776824951172\n",
            "loss = 65.18731689453125\n",
            "loss = 65.20298767089844\n",
            "loss = 65.17527770996094\n",
            "loss = 65.16818237304688\n",
            "loss = 65.16487121582031\n",
            "loss = 65.17860412597656\n",
            "loss = 65.16209411621094\n",
            "loss = 65.16246795654297\n",
            "loss = 65.174072265625\n",
            "loss = 65.18830871582031\n",
            "loss = 65.17294311523438\n",
            "loss = 65.16397094726562\n",
            "loss = 65.17121124267578\n",
            "loss = 65.16230773925781\n",
            "loss = 65.17404174804688\n",
            "loss = 65.16978454589844\n",
            "loss = 65.17945098876953\n",
            "loss = 65.18814849853516\n",
            "loss = 65.21514129638672\n",
            "loss = 65.20079803466797\n",
            "loss = 65.18273162841797\n",
            "loss = 65.16521453857422\n",
            "loss = 65.17957305908203\n",
            "loss = 65.17137145996094\n",
            "loss = 65.16722106933594\n",
            "loss = 65.17327117919922\n",
            "loss = 65.18622589111328\n",
            "loss = 65.18515014648438\n",
            "loss = 65.16490173339844\n",
            "loss = 65.17359161376953\n",
            "loss = 65.17559814453125\n",
            "loss = 65.17950439453125\n",
            "loss = 65.16051483154297\n",
            "loss = 65.1671142578125\n",
            "loss = 65.1937255859375\n",
            "loss = 65.1749496459961\n",
            "loss = 65.20623779296875\n",
            "loss = 65.208251953125\n",
            "loss = 65.1963119506836\n",
            "loss = 65.17739868164062\n",
            "loss = 65.1861343383789\n",
            "loss = 65.18134307861328\n",
            "loss = 65.18058013916016\n",
            "loss = 65.16095733642578\n",
            "loss = 65.17449188232422\n",
            "loss = 65.20699310302734\n",
            "loss = 65.19881439208984\n",
            "loss = 65.17996978759766\n",
            "loss = 65.16041564941406\n",
            "loss = 65.1915512084961\n",
            "loss = 65.19536590576172\n",
            "loss = 65.18157196044922\n",
            "loss = 65.176025390625\n",
            "loss = 65.1818618774414\n",
            "loss = 65.18595123291016\n",
            "loss = 65.18714904785156\n",
            "loss = 65.1772232055664\n",
            "loss = 65.16912078857422\n",
            "loss = 65.1628646850586\n",
            "loss = 65.177490234375\n",
            "loss = 65.18528747558594\n",
            "loss = 65.18327331542969\n",
            "loss = 65.18161010742188\n",
            "loss = 65.16490173339844\n",
            "loss = 65.18305206298828\n",
            "loss = 65.16876983642578\n",
            "loss = 65.17288208007812\n",
            "loss = 65.16839599609375\n",
            "loss = 65.16085815429688\n",
            "loss = 65.20220947265625\n",
            "loss = 65.19459533691406\n",
            "loss = 65.18596649169922\n",
            "loss = 65.16769409179688\n",
            "loss = 65.17276000976562\n",
            "loss = 65.16532135009766\n",
            "loss = 65.17070770263672\n",
            "loss = 65.17100524902344\n",
            "loss = 65.16207885742188\n",
            "loss = 65.19004821777344\n",
            "loss = 65.17922973632812\n",
            "loss = 65.19196319580078\n",
            "loss = 65.16596221923828\n",
            "loss = 65.18435668945312\n",
            "loss = 65.1834487915039\n",
            "loss = 65.19754028320312\n",
            "loss = 65.17363739013672\n",
            "loss = 65.18009948730469\n",
            "loss = 65.22516632080078\n",
            "loss = 65.20532989501953\n",
            "loss = 65.23558807373047\n",
            "loss = 65.21613311767578\n",
            "loss = 65.1828842163086\n",
            "loss = 65.17291259765625\n",
            "loss = 65.1886215209961\n",
            "loss = 65.20997619628906\n",
            "loss = 65.1817398071289\n",
            "loss = 65.19447326660156\n",
            "loss = 65.210693359375\n",
            "loss = 65.23335266113281\n",
            "loss = 65.1799545288086\n",
            "loss = 65.17225646972656\n",
            "loss = 65.18671417236328\n",
            "loss = 65.21219635009766\n",
            "loss = 65.20010375976562\n",
            "loss = 65.15768432617188\n",
            "loss = 65.21243286132812\n",
            "loss = 65.24903106689453\n",
            "loss = 65.2437973022461\n",
            "loss = 65.17546081542969\n",
            "loss = 65.19056701660156\n",
            "loss = 65.24247741699219\n",
            "loss = 65.23088836669922\n",
            "loss = 65.1771240234375\n",
            "loss = 65.18467712402344\n",
            "loss = 65.22943115234375\n",
            "loss = 65.21508026123047\n",
            "loss = 65.18310546875\n",
            "loss = 65.17194366455078\n",
            "loss = 65.19052124023438\n",
            "loss = 65.18012237548828\n",
            "loss = 65.16487121582031\n",
            "loss = 65.17350006103516\n",
            "loss = 65.19878387451172\n",
            "loss = 65.20763397216797\n",
            "loss = 65.19092559814453\n",
            "loss = 65.18113708496094\n",
            "loss = 65.17291259765625\n",
            "loss = 65.18648529052734\n",
            "loss = 65.16824340820312\n",
            "loss = 65.16191864013672\n",
            "loss = 65.16857147216797\n",
            "loss = 65.1778793334961\n",
            "loss = 65.17671966552734\n",
            "loss = 65.1726303100586\n",
            "loss = 65.17669677734375\n",
            "loss = 65.1580810546875\n",
            "loss = 65.1826400756836\n",
            "loss = 65.17448425292969\n",
            "loss = 65.17982482910156\n",
            "loss = 65.1899185180664\n",
            "loss = 65.18972778320312\n",
            "loss = 65.20541381835938\n",
            "loss = 65.1875991821289\n",
            "loss = 65.19393157958984\n",
            "loss = 65.18861389160156\n",
            "loss = 65.18490600585938\n",
            "loss = 65.1797866821289\n",
            "loss = 65.19792175292969\n",
            "loss = 65.17964935302734\n",
            "loss = 65.1684799194336\n",
            "loss = 65.2081527709961\n",
            "loss = 65.18167114257812\n",
            "loss = 65.17125701904297\n",
            "loss = 65.17425537109375\n",
            "loss = 65.18511199951172\n",
            "loss = 65.16812133789062\n",
            "loss = 65.17166137695312\n",
            "loss = 65.17253875732422\n",
            "loss = 65.15888977050781\n",
            "loss = 65.1973648071289\n",
            "loss = 65.16881561279297\n",
            "loss = 65.19734954833984\n",
            "loss = 65.17486572265625\n",
            "loss = 65.16460418701172\n",
            "loss = 65.1628189086914\n",
            "loss = 65.17546844482422\n",
            "loss = 65.18116760253906\n",
            "loss = 65.1739273071289\n",
            "loss = 65.19347381591797\n",
            "loss = 65.19576263427734\n",
            "loss = 65.19403076171875\n",
            "loss = 65.16202545166016\n",
            "loss = 65.18279266357422\n",
            "loss = 65.18716430664062\n",
            "loss = 65.21124267578125\n",
            "loss = 65.17611694335938\n",
            "loss = 65.16629791259766\n",
            "loss = 65.21434783935547\n",
            "loss = 65.23234558105469\n",
            "loss = 65.2183609008789\n",
            "loss = 65.18431091308594\n",
            "loss = 65.20849609375\n",
            "loss = 65.22080993652344\n",
            "loss = 65.19795227050781\n",
            "loss = 65.16400146484375\n",
            "loss = 65.18612670898438\n",
            "loss = 65.20613861083984\n",
            "loss = 65.19342041015625\n",
            "loss = 65.17889404296875\n",
            "loss = 65.16900634765625\n",
            "loss = 65.20051574707031\n",
            "loss = 65.17818450927734\n",
            "loss = 65.16217803955078\n",
            "loss = 65.16985321044922\n",
            "loss = 65.1875991821289\n",
            "loss = 65.22576904296875\n",
            "loss = 65.18769836425781\n",
            "loss = 65.192626953125\n",
            "loss = 65.18709564208984\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for i in range(1000):\n",
        "    # feed data through model to get prediction\n",
        "    y = model(x)\n",
        "\n",
        "    # calculate loss (compare prediction to true value)\n",
        "    loss = loss_fn(y,yt)\n",
        "\n",
        "    # calculate gradients of parameters\n",
        "    loss.backward()\n",
        "\n",
        "    #make the parameter to take a step -> update the parameter\n",
        "    optimizer.step()\n",
        "\n",
        "    # clear gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    losses+=[loss.item()]\n",
        "    print( f\"loss = {loss}\")\n",
        "plt.plot(losses);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRlkcpT0D-a1"
      },
      "source": [
        "## MNIST Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "F4GfdMRhD-a1"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "fyQ4gP46D-a3"
      },
      "outputs": [],
      "source": [
        "data = MNIST(\".\",download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "wdfX2XnGD-a4",
        "outputId": "5b4c9bd0-8288-490d-a675-0f9bbfbdce6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "s7fwSvMkD-a5",
        "outputId": "9eebf0c9-0c3d-4cdb-de12-5384f00a8d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAe0lEQVR4nGNgGHhge8ALt+TfP/o45ZL+XuJG8JhQJTkYrn3FKcnAsAK3lVf+2+OUM/h0TwCJi2qsANfXDzglGRh2MuCRvIdb0o6REad7GFb+kcapk4md4SlOjWJ//qDw0R2ki1NSkJFRB6ex9QSMJV4SBcjdXEK0WloBAMPfGW/ZC89pAAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 147
        }
      ],
      "source": [
        "import numpy as np\n",
        "img,y = data[np.random.randint(1,60000)]\n",
        "print(y)\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "qSkUnhyaWfGG",
        "outputId": "9636f36f-b90d-47bc-f3ce-80850c3659a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGArA+YU6AwMDAwMTAwMDg10gqqTpGQaEpEMQihyTohwjgndnMYqk9L9FSDqZUE2dw3AbIaknjirJz7AbIenFiSInrsjwFCGpznAVWbJH/NZnCIuFgYGBgeE0XIbPI8aNofkDsqQQAwODPpOzDFs00/eTP1nOQlUyMjAwTEv/8IiBQY/xz7drJ88cfPlEkI0BoTProRUDA8OjjddOMDAwMKSJ3mPACVb+64QxmbBIb8AnyYBHklEVj+R/JjySDJb4jMVj5/b/OB1IJQAAg3ksR3QPgSAAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 148
        }
      ],
      "source": [
        "img,y = data[2]\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "3giZTnRiD-a-",
        "outputId": "9bb83098-eaf2-40e3-f07e-5467dd185607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(data.data[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "nsZHdAMlD-a_",
        "outputId": "fd2d3539-fefe-45ad-f7b7-f634ab22517c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4)\n"
          ]
        }
      ],
      "source": [
        "print(data.targets[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROmpaSMVD-bA"
      },
      "source": [
        "### MNIST Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "qHhKarpfD-bB"
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Sequential( # 28*28 = 784\n",
        "    torch.nn.Linear(784,  100),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(100, 100),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(100, 10),\n",
        "    torch.nn.LogSoftmax(dim=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "E8Q59QCED-bC"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "ehneQZatD-bG"
      },
      "outputs": [],
      "source": [
        "sample = np.random.choice(range(len(data.data)),1000)\n",
        "x = data.data[sample].reshape(1000,-1).float()/255\n",
        "yt = data.targets[sample]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "ucZS9UPAD-bJ",
        "outputId": "07ddb2e0-948c-4915-fec0-26e8f7e2a66e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1000, 784]), torch.Size([1000]))"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ],
      "source": [
        "x.shape,yt.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "jdOxW-tYD-bK"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "mMtNhgaED-bL",
        "outputId": "2aa96453-5047-498d-95ab-96548edabcbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for i in range(100):\n",
        "    # get data and true labels\n",
        "    sample = np.random.choice(range(len(data.data)),1000)\n",
        "    x = data.data[sample].reshape(1000,-1).float()/255\n",
        "    yt = data.targets[sample]\n",
        "\n",
        "    # get prediction\n",
        "    y = model(x)\n",
        "\n",
        "    # input (Tensor) – Predicted unnormalized logits\n",
        "    # target (Tensor) – Ground truth class indices or class probabilities\n",
        "    loss = loss_fn(y,yt)\n",
        "\n",
        "    # compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters and clear previous gradients\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    losses+=[loss.item()]\n",
        "    #print( f\"loss = {loss}\")\n",
        "plt.plot(losses);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "1BAXeaKHD-bM"
      },
      "outputs": [],
      "source": [
        "x_test = data.data[-1000:].reshape(1000,-1).float()/255\n",
        "y_test = data.targets[-1000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "5VXFOBCjD-bO"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    y_pred = model(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "Ms69eUDYYMGj",
        "outputId": "cdf1925e-dafb-42b5-9202-35c4b438bc54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2)"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ],
      "source": [
        "y_test[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "6oG-bennMI3Y",
        "outputId": "61da10c9-fdb5-4e8f-fe6c-858805386158",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2)"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ],
      "source": [
        "y_pred.argmax(dim=1)[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "tt0c-JhpD-bP",
        "outputId": "da9c64f5-6658-4159-b95d-056ed745310d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy =  0.973\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy = \", (y_pred.argmax(dim=1) == y_test).sum().float().item()/1000.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY6m7c0qD-bQ"
      },
      "source": [
        "## Course Conclusion\n",
        "\n",
        "1. By now you should have a sufficient introduction to the various ways one can use python for scientific computing. The best way to learn more is to start using python for whatever project you are working on. Only practice will make you comfortable with using python.   \n",
        "\n",
        "Recommended Project Source: kaggle  \n",
        "\n",
        "Recommended ML/DL Courses: CS229, 230, 231N, 224 series, 238, 246  \n",
        "\n",
        "2. I appreciate your time to submit the course feedback, which means a lot to me and improvement for this course in the future  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pDnSshCXD-ZL",
        "2JDTirUfD-Zf",
        "vICVTE1wD-Zq",
        "wDNQLaL6D-Zu",
        "hNbj9oDlD-aS",
        "3kRdaQe6D-ab",
        "5t7iYj2eD-an",
        "tRlkcpT0D-a1",
        "ROmpaSMVD-bA",
        "VY6m7c0qD-bQ"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "PyTorch-GPU",
      "language": "python",
      "name": "pyt-gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}